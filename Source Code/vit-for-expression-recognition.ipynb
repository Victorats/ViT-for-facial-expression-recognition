{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":10219242,"sourceType":"datasetVersion","datasetId":6317198}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Vision Transformer for Facial Expression Recognition\n\n# Final Project for Deep Learning Course\n# Students: Gabriel Moreira Marques - 108207\n# Victor Afonso Teixeira Santos - 108212\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport math\nimport torch.nn.functional as F\nimport os\nfrom tqdm import tqdm\n\ndatasetDir = '/kaggle/input/meu-tinyimgnet/tiny-imagenet-200'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\noutputDir = '/kaggle/working/tiny-imagenet-200'\nval_images_dir = os.path.join(datasetDir, 'val', 'images')\nval_annotations_path = os.path.join(datasetDir, 'val', 'val_annotations.txt')\nval_output_dir = os.path.join(outputDir, 'val', 'organized')\n\ntransform = transforms.Compose([\n    transforms.Resize(64),\n    transforms.CenterCrop(64),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nif not os.path.exists(outputDir):\n    print(\"Copiando o dataset para o diretório de trabalho...\")\n    shutil.copytree(datasetDir, outputDir)\n\n#organizando as pastas\nif not os.path.exists(val_output_dir):\n    os.makedirs(val_output_dir, exist_ok=True)\n    with open(val_annotations_path, 'r') as f:\n        for line in f.readlines():\n            parts = line.strip().split('\\t')  \n            image_name, class_id = parts[0], parts[1]\n            \n            class_dir = os.path.join(val_output_dir, class_id)\n            os.makedirs(class_dir, exist_ok=True)\n\n            src = os.path.join(val_images_dir, image_name)\n            dst = os.path.join(class_dir, image_name)\n            shutil.copy(src, dst)\nelse:\n    print(\"O conjunto de validação já está organizado.\")\n\n\ntrainSet = datasets.ImageFolder(root=os.path.join(outputDir, 'train'), transform=transform)\nvalSet = datasets.ImageFolder(root=val_output_dir, transform=transform)\n\nbatchSize = 32\ntrain_loader = DataLoader(trainSet, batch_size=batchSize, shuffle=True, num_workers=2)\nval_loader = DataLoader(valSet, batch_size=batchSize, shuffle=False, num_workers=2)\n\nprint(f\"Quantidade de imagens de treino: {len(trainSet)}\")\nprint(f\"Quantidade de imagens de validação: {len(valSet)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\nlearning_rate = 1e-4\n#1e-3 pode ser alto para ViTs, especialmente no início do treinamento.\n#Normalmente, taxas menores, como 1e-4 ou 3e-4, são mais estáveis. O artigo sugere o uso de linear warmup e decaimento (cosine).\nn_outputs = 200 #se for treinar com o tiny imagenet trocar pra 200(afinal tem 200 classes) #valor p expressoes 7\npatch_size = 8\nimage_size = 64\ninput_channels = 3 #tinyimagenet e os do fine tunning usam imagenns rgb, entao 3 canais\nn_heads = 4\ndropout = 0.1\nhidden_dim = 768\nadam_weight_decay = 0.1\nbetas = (0.9, 0.999)\nn_encoders = 8\n# (n_encoders=2) é baixo. Modelos ViT geralmente têm mais camadas (como 6, 12 ou mais) para aprender representações mais profundas.\nembed_dim = (patch_size ** 2) * input_channels\nnum_epochs = 14","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim)\n        super().__init__()\n        \n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        self.embed_dim = embed_dim\n\n        self.projection = nn.Conv2d(\n            in_channels, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        batch_size, channels, height, width = x.shape\n\n        x = self.projection(x)  # [batch_size, embed_dim, num_patches^(1/2), num_patches^(1/2)]\n\n        x = x.flatten(2).transpose(1, 2)#-> [batch_size, num_patches, embed_dim]\n        return x\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, num_patches, embed_dim):\n        super().__init__()\n        \n        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n\n    def forward(self, x):\n        return x + self.position_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VisionTransformerInput(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n        self.pos_embed = PositionalEmbedding(self.patch_embed.num_patches, embed_dim)\n\n    def forward(self, x):\n        x = self.patch_embed(x)  # Gera os embeddings dos patches\n        x = self.pos_embed(x)    # Adiciona os embeddings posicionais\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n\n        super().__init__()\n\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = math.sqrt(self.head_dim)\n\n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.attention_dropout = nn.Dropout(dropout)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x):\n        batch_size, num_patches, embed_dim = x.size()\n\n        \n        qkv = self.qkv(x)  # [batch_size, num_patches, 3 * embed_dim]\n        qkv = qkv.reshape(batch_size, num_patches, 3, self.num_heads, self.head_dim)\n        q, k, v = qkv.permute(2, 0, 3, 1, 4) \n\n        attn_weights = (q @ k.transpose(-2, -1)) / self.scale  # [batch_size, num_heads, num_patches, num_patches]\n        attn_weights = attn_weights.softmax(dim=-1)\n        attn_weights = self.attention_dropout(attn_weights)\n\n        attn_output = (attn_weights @ v).transpose(1, 2).reshape(batch_size, num_patches, embed_dim)\n        return self.proj(attn_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerEncoderBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, embed_dim),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        # atenção + Residual\n        x = x + self.attn(self.norm1(x))\n        # feedforward + Residual\n        x = x + self.mlp(self.norm2(x))\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n\n        super().__init__()\n        self.input_layer = VisionTransformerInput(image_size, patch_size, in_channels, embed_dim)\n        self.encoder = nn.ModuleList([\n            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)\n        ])\n        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.input_layer(x)\n\n        class_token = self.class_token.expand(batch_size, -1, -1)\n        x = torch.cat((class_token, x), dim=1)\n\n        for layer in self.encoder:\n            x = layer(x)\n\n        return self.mlp_head(x[:, 0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch_with_progress(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss, correct, total_samples = 0, 0, 0\n    loop = tqdm(dataloader, desc=\"Treinando\", leave=False)\n\n    for images, labels in loop:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * images.size(0)\n        _, preds = outputs.max(1)\n        correct += (preds == labels).sum().item()\n        total_samples += images.size(0)\n\n        loop.set_postfix(loss=loss.item(), acc=(correct / total_samples))\n\n    avg_loss = total_loss / total_samples\n    accuracy = correct / total_samples\n    return avg_loss, accuracy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_one_epoch_with_progress(model, dataloader, criterion, device):\n    model.eval()\n    total_loss, correct, total_samples = 0, 0, 0\n\n    with torch.no_grad():\n        loop = tqdm(dataloader, desc=\"Validando\", leave=False)\n\n        for images, labels in loop:\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item() * images.size(0)\n            _, preds = outputs.max(1)\n            correct += (preds == labels).sum().item()\n            total_samples += images.size(0)\n\n            loop.set_postfix(loss=loss.item(), acc=(correct / total_samples))\n\n    avg_loss = total_loss / total_samples\n    accuracy = correct / total_samples\n    return avg_loss, accuracy\n\n\ndef train_and_save(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, save_dir):\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        print(\"-\" * 40)\n\n        train_loss, train_acc = train_one_epoch_with_progress(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = validate_one_epoch_with_progress(model, val_loader, criterion, device)\n\n        scheduler.step()\n\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n\n        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n\n        # Salva checkpoint\n        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch + 1}.pt')\n        torch.save({'epoch': epoch + 1,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'loss': val_loss},\n                   checkpoint_path)\n        print(f\"Checkpoint salvo: {checkpoint_path}\\n\")\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Validation Loss')\n    plt.plot(history['train_acc'], label='Train Accuracy')\n    plt.plot(history['val_acc'], label='Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Metrics')\n    plt.title('Training and Validation Loss/Accuracy')\n    plt.legend()\n    plt.savefig(os.path.join(save_dir, 'training_results.png'))\n    plt.show()\n    print(\"Treinamento concluído. Gráfico salvo.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim.lr_scheduler import CosineAnnealingLR\nimport torch.optim as optim\n\ndef execute_training_with_saving(train_loader, val_loader, save_dir):\n    model = VisionTransformer(\n        image_size=image_size,\n        patch_size=patch_size,\n        in_channels=input_channels,\n        embed_dim=hidden_dim,\n        num_heads=n_heads,\n        mlp_dim=hidden_dim * 4,\n        num_layers=n_encoders,\n        num_classes=n_outputs,\n        dropout=dropout\n    ).to(device)\n\n    # Função de perda e otimizador\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(\n        model.parameters(), lr=learning_rate, betas=betas, weight_decay=adam_weight_decay\n    )\n    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)  # Decaimento cosseno\n\n    train_and_save(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        num_epochs=num_epochs,\n        device=device,\n        save_dir=save_dir\n    )\n\nsave_directory = \"/kaggle/working/\" \nexecute_training_with_saving(train_loader, val_loader, save_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}